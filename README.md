# Day 39 â€“ AdaBoost: Classification & Regression (End-to-End)


## ğŸ“Œ Project Overview

This project is part of my **Daily Machine Learning Practice (Day 39)** and focuses on implementing **AdaBoost (Adaptive Boosting)** for both **classification** and **regression** problems. The goal is to understand how boosting improves model performance by combining multiple weak learners into a strong ensemble model.

---

##  ğŸš€ What is AdaBoost?

AdaBoost (Adaptive Boosting) is an **ensemble learning technique** that:
- Combines multiple **weak learners** (usually decision stumps)
- Assigns **higher weights to misclassified samples**
- Iteratively improves model accuracy
- Reduces bias and improves generalization

AdaBoost can be applied to:

- **Classification problems** â†’ `AdaBoostClassifier`
- **Regression problems** â†’ `AdaBoostRegressor`

---

##  ğŸ§  Objectives

- Understand the working principle of AdaBoost
- Implement AdaBoost for **classification**
- Implement AdaBoost for **regression**
- Compare model performance with evaluation metrics
- Build an end-to-end ML workflow

---

## ğŸ—‚ï¸ Project Structure

`Day_39_ML_Ada-Boost_Classification-Regression/
â”‚
â”œâ”€â”€ AdaBoost_Classification.ipynb
â”œâ”€â”€ AdaBoost_Regression.ipynb
â”œâ”€â”€ dataset/
â”‚ â”œâ”€â”€ classification_data.csv
â”‚ â””â”€â”€ regression_data.csv
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt`


---

##  ğŸ› ï¸ Tech Stack & Libraries
- **Programming Language:** Python  
- **Libraries Used:**
  - NumPy
  - Pandas
  - Matplotlib / Seaborn
  - Scikit-learn

---

##  ğŸ“Š AdaBoost Classification
### Workflow
1. Load and explore dataset
2. Perform data preprocessing
3. Split data into training & testing sets
4. Train `AdaBoostClassifier`
5. Evaluate model performance

###  Evaluation Metrics
- Accuracy Score
- Confusion Matrix
- Classification Report (Precision, Recall, F1-score)

---

##  ğŸ“ˆ AdaBoost Regression
### Workflow
1. Load and analyze regression dataset
2. Data preprocessing
3. Train `AdaBoostRegressor`
4. Make predictions
5. Evaluate results

###  Evaluation Metrics
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- RÂ² Score

---

##  âš™ï¸ Model Parameters Used
- `n_estimators`
- `learning_rate`
- `base_estimator` (Decision Tree Regressor / Classifier)

These parameters were tuned to observe their impact on model performance.

---

##  ğŸ“Œ Key Learnings
- AdaBoost focuses more on **hard-to-classify samples**
- Boosting reduces bias compared to single models
- Works well with weak learners
- Sensitive to noisy data and outliers

---

##  ğŸ“‰ Results & Observations
- AdaBoost significantly improved accuracy over baseline models
- Classification results showed improved recall and F1-score
- Regression model achieved better error reduction compared to single regressors

---

##  ğŸ”® Future Improvements
- Hyperparameter tuning using GridSearchCV
- Comparison with other ensemble methods (Random Forest, Gradient Boosting, XGBoost)
- Handling noisy data more effectively
- Feature importance visualization

---

##  ğŸ“š References
- Scikit-learn Documentation  
- Machine Learning by Andrew Ng  
- Hands-On Machine Learning with Scikit-Learn  

---

##  ğŸ‘¨â€ğŸ’» Author

**Laxman Bhimrao Khedkar**  
- LinkedIn: https://www.linkedin.com/in/laxman-khedkar  
- GitHub: https://github.com/laxmankhedkar  
- Portfolio: https://beacons.ai/laxmankhedkar  

---
